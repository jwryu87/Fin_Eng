##################################################
# import
##################################################
import bs4                                                                      # 웹크롤링에 사용하는 BeautifulSoup import
import datetime as dt                                                           # 날짜 형식 작업

##################################################
# url
##################################################
# url 주소 가져오기
index_cd = 'KPI200'  # 변수 index_cd
page_n = 1           # 변수 page_n
naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)  # 변수 naver_index

# url 실제 소스 가져오기 *****
from urllib.request import urlopen  
source = urlopen(naver_index).read()
source = bs4.BeautifulSoup(source, 'lxml')                                      # url 소스 예쁘게 가져오기: bs4.BeautifulSoup
# print(source.prettify())

# url에서 td의 개수
# td = source.find_all('td')
# len(td)

##################################################
# 날짜추출
##################################################

# /html/body/div/table[1]/tbody/tr[3]/td[1]
# source.find_all('table')[0].find_all('tr')[2].find_all('td')[0]

d = source.find_all('td', class_='date')[0].text                                # XPath 확인
def date_format(d):                                                             # 날짜 추출 함수
    d = str(d).replace('-', '.')
    yyyy = int(d.split('.')[0]) 
    mm = int(d.split('.')[1])
    dd = int(d.split('.')[2])
    this_date= dt.date(yyyy, mm, dd)
    return this_date

##################################################
# 종가추출
##################################################

# /html/body/div/table[1]/tbody/tr[3]/td[2]    
this_close = source.find_all('tr')[2].find_all('td')[1].text                    # bs4의 find_all 을 사용
this_close = this_close.replace(',', '')                                        # ','이 있다면 ''로 replace
this_close = float(this_close)                                                  # float으로 데이터형 변환
this_close

#p = source.find_all('td', class_='number_1')[0].text                            # 위와 같은 거

dates = source.find_all('td', class_='date')                                    # 페이지 상의 날짜 추출
prices = source.find_all('td', class_='number_1')                               # 페이지 상의 종가정보 전체 추출

#len(dates)                                                                      # 문자열 길이 구하기
#len(prices)

for n in range(len(dates)):
    this_date = dates[n].text
    this_date = date_format(this_date)
    
    this_close = prices[n*4].text   
    # 0, 4, 8, ... 4의 배수로 돌아가는 가격 추출
    this_close = this_close.replace(',', '')
    this_close = float(this_close)
    this_close
    
    print(this_date, this_close)



##################################################
# 마지막 페이지 번호 찾기
##################################################

# /html/body/div/table[2]/tbody/tr/td[7]/a
# paging = source.find('td', class_='pgRR').find('a')['href']                     # 가장 마지막 페이지 url 가져오기
# paging = paging.split('&')[1]                                                   # paging 변수를 & 기준으로 나누고 2번째 항목을 가져온다
# paging = paging.split('=')[1]                                                   # paging 변수를 = 기준으로 나누고 2번째 항목을 가져온다
# paging

naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(505)  # naver_index 변수를 다시 지정해주고 
source = urlopen(naver_index).read()
source = bs4.BeautifulSoup(source, 'lxml')                                      # url 소스 예쁘게 가져오기: bs4.BeautifulSoup

if source.find('td', class_='pgRR'):                                            # 가장 마지막 페이지 주소를 하이퍼링크 <a> 태그는 <td class="pgRR"> 이다.
    last_page = source.find('td', class_='pgRR').find('a')['href']
    last_page = last_page.split('&')[1]
    last_page = last_page.split('=')[1]
    last_page = int(last_page)



##################################################
# main 데이터 추출 기능 함수
##################################################

def historical_index_naver(index_cd, page_n=1, last_page=0):   
        
    naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)
    
    source = urlopen(naver_index).read()   # 지정한 페이지에서 코드 읽기
    source = bs4.BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류
    
    dates = source.find_all('td', class_='date')   # <td class="date">태그에서 날짜 수집   
    prices = source.find_all('td', class_='number_1')   # <td class="number_1">태그에서 지수 수집
    
    for n in range(len(dates)):
    
        if dates[n].text.split('.')[0].isdigit():
            
            # 날짜 처리
            this_date = dates[n].text
            this_date= date_format(this_date)
            
            # 종가 처리
            this_close = prices[n*4].text   # prices 중 종가지수인 0,4,8,...번째 데이터 추출
            this_close = this_close.replace(',', '')
            this_close = float(this_close)

            # 딕셔너리에 저장
            historical_prices[this_date] = this_close
            
    # 페이지 네비게이션
    if last_page == 0:
        last_page = source.find('td', class_='pgRR').find('a')['href']
        # 마지막페이지 주소 추출
        last_page = last_page.split('&')[1]   # & 뒤의 page=506 부분 추출
        last_page = last_page.split('=')[1]   # = 뒤의 페이지번호만 추출
        last_page = int(last_page)   # 숫자형 변수로 변환
        
    # 다음 페이지 호출
    if page_n < last_page:   
        page_n = page_n + 1   
        historical_index_naver(index_cd, start_date, end_date, page_n, last_page)   
        
    return historical_prices
    


##################################################
# main_2
##################################################

# 네이버에서 일자별 인덱스를 추출하는 함수 정의

def historical_index_naver(index_cd, start_date='', end_date='', page_n=1, last_page=0):
    
    if start_date:   # start_date가 있으면
        start_date = date_format(start_date)   # date 포맷으로 변환
    else:    # 없으면
        start_date = dt.date.today()   # 오늘 날짜를 지정
    if end_date:   
        end_date = date_format(end_date)   
    else:   
        end_date = dt.date.today()  
        
        
    naver_index = 'http://finance.naver.com/sise/sise_index_day.nhn?code=' + index_cd + '&page=' + str(page_n)
    
    source = urlopen(naver_index).read()   # 지정한 페이지에서 코드 읽기
    source = bs4.BeautifulSoup(source, 'lxml')   # 뷰티풀 스프로 태그별로 코드 분류
    
    dates = source.find_all('td', class_='date')   # <td class="date">태그에서 날짜 수집   
    prices = source.find_all('td', class_='number_1')   # <td class="number_1">태그에서 지수 수집
    
    for n in range(len(dates)):
    
        if dates[n].text.split('.')[0].isdigit():
            
            # 날짜 처리
            this_date = dates[n].text
            this_date= date_format(this_date)
            
            if this_date <= end_date and this_date >= start_date:   
            # start_date와 end_date 사이에서 데이터 저장
                # 종가 처리
                this_close = prices[n*4].text   # prices 중 종가지수인 0,4,8,...번째 데이터 추출
                this_close = this_close.replace(',', '')
                this_close = float(this_close)

                # 딕셔너리에 저장
                historical_prices[this_date] = this_close
                
            elif this_date < start_date:   
            # start_date 이전이면 함수 종료
                return historical_prices              
            
    # 페이지 네비게이션
    if last_page == 0:
        last_page = source.find('td', class_='pgRR').find('a')['href']
        # 마지막페이지 주소 추출
        last_page = last_page.split('&')[1]   # & 뒤의 page=506 부분 추출
        last_page = last_page.split('=')[1]   # = 뒤의 페이지번호만 추출
        last_page = int(last_page)   # 숫자형 변수로 변환
        
    # 다음 페이지 호출
    if page_n < last_page:   
        page_n = page_n + 1   
        historical_index_naver(index_cd, start_date, end_date, page_n, last_page)   
        
    return historical_prices
    
index_cd = 'KPI200'
historical_prices = dict()
historical_index_naver(index_cd, '2018-4-1', '2018-4-4')
historical_prices